{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(150 unique tokens: [u'\\u5f85', u'\\u0e04', u'\\xac', u'\\u0e14', u'\\u6599']...)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script\n",
    "* Loads documents as aggregation of tweets stored in a MongoDB collection\n",
    "* Cleans up the documents\n",
    "* Creates a dictionary and corpus that can be used to train an LDA model\n",
    "* Training of the LDA model is not included but follows:\n",
    "  lda = models.LdaModel(corpus, id2word=dictionary, num_topics=100, passes=100)\n",
    "Author: Alex Perrier\n",
    "Python 2.7\n",
    "'''\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "\n",
    "# Gensim is used for LDA and word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import codecs\n",
    "import langid\n",
    "# import nltk\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from configparser import ConfigParser\n",
    "from gensim import corpora, models, similarities\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "from pymongo import MongoClient\n",
    "from string import digits\n",
    "\n",
    "# Loading the tweet data\n",
    "filename = '../../assets/dataset/captured-tweets.txt'\n",
    "\n",
    "tweets = []\n",
    "for tweet in codecs.open(filename, 'r', encoding=\"utf-8\"):\n",
    "    tweets.append(tweet)\n",
    "\n",
    "# Setting up spacy\n",
    "nlp_toolkit = English()\n",
    "\n",
    "\n",
    "\n",
    "# connect to the MongoDB\n",
    "client      = MongoClient()\n",
    "db          = client['twitter']\n",
    "\n",
    "# Load documents and followers from db\n",
    "# Filter out non-english timelines and TL with less than 2 tweets\n",
    "# documents    = [tw['raw_text'] for tw in db.tweets.find()\n",
    "#                    if ('lang' in tw.keys()) and (tw['lang'] in ('en','und'))\n",
    "#                        and ('n_tweets' in tw.keys()) and (tw['n_tweets'] > 2) ]\n",
    "\n",
    "documents = tweets\n",
    "\n",
    "# Remove urls\n",
    "documents = [re.sub(r\"(?:\\@|http?\\://)\\S+\", \"\", doc)\n",
    "                for doc in documents ]\n",
    "\n",
    "# Remove documents with less 100 words (some timeline are only composed of URLs)\n",
    "documents = [doc for doc in documents if len(doc) > 100]\n",
    "\n",
    "# tokenize\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# documents = [ tokenizer.tokenize(doc.lower()) for doc in documents ]\n",
    "\n",
    "# Remove stop words\n",
    "stoplist_tw=['amp','get','got','hey','hmm','hoo','hop','iep','let','ooo','par',\n",
    "            'pdt','pln','pst','wha','yep','yer','aest','didn','nzdt','via',\n",
    "            'one','com','new','like','great','make','top','awesome','best',\n",
    "            'good','wow','yes','say','yay','would','thanks','thank','going',\n",
    "            'new','use','should','could','best','really','see','want','nice',\n",
    "            'while','know']\n",
    "\n",
    "unigrams = [ w for doc in documents for w in doc if len(w)==1]\n",
    "bigrams  = [ w for doc in documents for w in doc if len(w)==2]\n",
    "\n",
    "# stoplist  = set(nltk.corpus.stopwords.words(\"english\") + stoplist_tw\n",
    "#                 + unigrams + bigrams)\n",
    "#documents = [[token for token in doc if token not in stoplist]\n",
    "#                for doc in documents]\n",
    "\n",
    "# rm numbers only words\n",
    "documents = [ [token for token in doc if len(token.strip(digits)) == len(token)]\n",
    "                for doc in documents ]\n",
    "\n",
    "# Lammetization\n",
    "# This did not add coherence ot the model and obfuscates interpretability of the\n",
    "# Topics. It was not used in the final model.\n",
    "#   from nltk.stem import WordNetLemmatizer\n",
    "#   lmtzr = WordNetLemmatizer()\n",
    "#   documents=[[lmtzr.lemmatize(token) for token in doc ] for doc in documents]\n",
    "\n",
    "# Remove words that only occur once\n",
    "token_frequency = defaultdict(int)\n",
    "\n",
    "# count all token\n",
    "for doc in documents:\n",
    "    for token in doc:\n",
    "        token_frequency[token] += 1\n",
    "\n",
    "# keep words that occur more than once\n",
    "documents = [ [token for token in doc if token_frequency[token] > 1]\n",
    "                for doc in documents  ]\n",
    "\n",
    "# Sort words in documents\n",
    "for doc in documents:\n",
    "    doc.sort()\n",
    "\n",
    "# Build a dictionary where for each document each word has its own id\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.compactify()\n",
    "# and save the dictionary for future use\n",
    "dictionary.save('alexip_followers_py27.dict')\n",
    "\n",
    "# We now have a dictionary with 26652 unique tokens\n",
    "print(dictionary)\n",
    "\n",
    "# Build the corpus: vectors with occurence of each word for each document\n",
    "# convert tokenized documents to vectors\n",
    "#corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# and save in Market Matrix format\n",
    "#corpora.MmCorpus.serialize('alexip_followers_py27.mm', corpus)\n",
    "# this corpus can be loaded with corpus = corpora.MmCorpus('alexip_followers.mm')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
