{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT-NYC-37 | Lab 15 | Natural Language Processing and Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import feature_extraction, ensemble, cross_validation, metrics\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is about sentiments on Amazon reviews. The data is in a \"raw\" format where the review and it's score are separated by tabs (`\\t` character). We'll first need to parse it.\n",
    "\n",
    "Here's a sample:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "<pre>\n",
    "\\tIt clicks into place in a way that makes you wonder how long that mechanism would last.\\t0\n",
    "\\tI went on Motorola's website and followed all directions, but could not get it to pair again.\\t0\n",
    "</pre>\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "sentiments = []\n",
    "\n",
    "with open(os.path.join('..', 'datasets', 'amazon-reviews.txt')) as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        review, sentiment = line.split('\\t')\n",
    "        sentiment = np.nan if sentiment == '' else int(sentiment)\n",
    "\n",
    "        reviews.append(review)\n",
    "        sentiments.append(sentiment)\n",
    "\n",
    "df = pd.DataFrame({'review': reviews, 'sentiment': sentiments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I try not to adjust the volume setting to avoi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I thought Motorola made reliable products!.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Battery for Motorola Razr.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I try not to adjust the volume setting to avoi...        NaN\n",
       "1  So there is no way for me to plug it in here i...        0.0\n",
       "2                        Good case, Excellent value.        1.0\n",
       "3        I thought Motorola made reliable products!.        NaN\n",
       "4                         Battery for Motorola Razr.        NaN"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_temp = df.dropna(inplace = True) # Let's drop NaNs\n",
    "\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  sentiment\n",
       "1   So there is no way for me to plug it in here i...        0.0\n",
       "2                         Good case, Excellent value.        1.0\n",
       "5                              Great for the jawbone.        1.0\n",
       "10  Tied to charger for conversations lasting more...        0.0\n",
       "11                                  The mic is great.        1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.review\n",
    "y = df.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part I: Modeling text features using `CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` converts a collection of text into a matrix of features.  Each row will be a sample (an article or piece of text) and each column will be a text feature (usually a count or binary feature per word).\n",
    "\n",
    "**`CountVectorizer` takes a column of text and creates a new dataset.**  It generates a feature for every word in all of the pieces of text.\n",
    "\n",
    "CAUTION: Using all of the words can be useful, but we may need to use regularization to avoid overfitting.  Otherwise, rare words may cause the model to overfit and not generalize.\n",
    "\n",
    "(And check http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Defining and transforming the input using `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: \n",
      "[u'10', u'100', u'11', u'12', u'13', u'15', u'15g', u'18', u'20', u'2000', u'2005', u'2160', u'24', u'2mp', u'325', u'350', u'375', u'3o', u'42', u'44', u'45', u'4s', u'50', u'5020', u'510', u'5320', u'680', u'700w', u'8125', u'8525', u'8530', u'abhor', u'ability', u'able', u'abound', u'absolutel', u'absolutely', u'ac', u'accept', u'acceptable', u'access', u'accessory', u'accessoryone', u'accidentally', u'accompanied', u'actually', u'ad', u'adapter', u'adapters', u'add', u'addition', u'additional', u'address', u'adhesive', u'adorable', u'advertised', u'advise', u'aggravating', u'ago', u'alarm', u'allot', u'allow', u'allowing', u'allows', u'alot', u'aluminum', u'amazed', u'amazing', u'amazon', u'amp', u'ample', u'angeles', u'angle', u'answer', u'ant', u'antena', u'anti', u'apart', u'apartment', u'apparently', u'appealing', u'appearance', u'appears', u'applifies', u'appointments', u'area', u'arguing', u'armband', u'arrival', u'arrived', u'asia', u'ask', u'aspect', u'assumed', u'atleast', u'att', u'audio', u'auto', u'avoid', u'away', u'awesome', u'awful', u'background', u'bad', u'bar', u'barely', u'bargain', u'bars', u'basically', u'batteries', u'battery', u'beautiful', u'beep', u'belt', u'best', u'better', u'beware', u'big', u'bit', u'black', u'blackberry', u'blue', u'bluetooth', u'book', u'bother', u'bought', u'brand', u'break', u'breaking', u'breaks', u'broke', u'broken', u'bt', u'button', u'buttons', u'buy', u'buyer', u'buying', u'buzzing', u'cable', u'calls', u'came', u'camera', u'car', u'care', u'careful', u'case', u'cases', u'catching', u'caused', u'cell', u'cellphone', u'certainly', u'charge', u'charged', u'charger', u'chargers', u'charging', u'charm', u'cheap', u'cheaper', u'chinese', u'choice', u'cingular', u'clarity', u'clear', u'clearly', u'clip', u'color', u'come', u'comes', u'comfort', u'comfortable', u'comfortably', u'coming', u'company', u'complaint', u'completely', u'computer', u'connected', u'connection', u'construction', u'consumer', u'contacts', u'conversation', u'conversations', u'cool', u'costs', u'couldn', u'couple', u'cover', u'coverage', u'crap', u'crisp', u'cumbersome', u'current', u'customer', u'cut', u'd807', u'data', u'date', u'day', u'days', u'dead', u'deal', u'decent', u'decision', u'defective', u'definitely', u'described', u'description', u'design', u'despite', u'device', u'dialing', u'did', u'didn', u'died', u'different', u'difficult', u'disappointed', u'disappointing', u'disappointment', u'display', u'docking', u'does', u'doesn', u'don', u'dont', u'download', u'drain', u'driving', u'drop', u'dropped', u'dropping', u'drops', u'dying', u'ear', u'earbud', u'eargels', u'earpiece', u'earpieces', u'ears', u'ease', u'easier', u'easily', u'easy', u'echo', u'effect', u'embarrassing', u'end', u'ended', u'especially', u'essentially', u'exactly', u'excellent', u'exchanged', u'excited', u'expect', u'expected', u'expensive', u'experience', u'experienced', u'extended', u'extra', u'extremely', u'face', u'failed', u'fails', u'fall', u'family', u'fantastic', u'far', u'fast', u'feature', u'features', u'feel', u'feels', u'feet', u'felt', u'figure', u'finally', u'fine', u'fit', u'fits', u'flash', u'flaw', u'flawlessly', u'flimsy', u'flip', u'forced', u'forever', u'forget', u'form', u'free', u'friendly', u'functionality', u'games', u'gels', u'gets', u'getting', u'given', u'glad', u'glasses', u'goes', u'going', u'good', u'got', u'gotten', u'graphics', u'great', u'hand', u'hands', u'handsfree', u'handy', u'happier', u'happy', u'hard', u'hate', u'having', u'headphones', u'headset', u'headsets', u'hear', u'helpful', u'high', u'highly', u'hold', u'holding', u'holds', u'holster', u'home', u'hoping', u'horrible', u'hour', u'hours', u'house', u'huge', u'igo', u'im', u'important', u'impressed', u'impressive', u'included', u'incredible', u'inside', u'install', u'instead', u'instructions', u'internet', u'iphone', u'ipod', u'issues', u'item', u'jabra', u'jawbone', u'job', u'joke', u'junk', u'just', u'kept', u'keyboard', u'keypad', u'kind', u'know', u'lacking', u'laptop', u'large', u'lasting', u'lasts', u'later', u'leather', u'left', u'let', u'lg', u'life', u'light', u'lightweight', u'like', u'line', u'listening', u'literally', u'little', u'll', u'lock', u'logitech', u'long', u'longer', u'look', u'looking', u'looks', u'lose', u'lost', u'lot', u'loud', u'love', u'loves', u'low', u'maintain', u'make', u'makes', u'making', u'market', u'match', u'mess', u'mic', u'microphone', u'minutes', u'mistake', u'mobile', u'model', u'money', u'month', u'months', u'moto', u'motorola', u'mp3', u'music', u'nearly', u'need', u'needed', u'network', u'new', u'nice', u'noise', u'nokia', u'normal', u'notice', u'noticed', u'number', u'numerous', u'obviously', u'oh', u'ok', u'old', u'operate', u'order', u'ordered', u'original', u'outlet', u'overall', u'override', u'owned', u'packaged', u'pair', u'paired', u'pairing', u'palm', u'particular', u'party', u'pay', u'pc', u'pda', u'people', u'perfectly', u'performance', u'periods', u'person', u'petroleum', u'phone', u'phones', u'photo', u'pics', u'picture', u'pictures', u'piece', u'pitiful', u'pixel', u'place', u'placed', u'places', u'plan', u'planning', u'plans', u'plantronics', u'plantronincs', u'plastic', u'play', u'player', u'players', u'plays', u'pleasantly', u'pleased', u'pleather', u'plenty', u'plug', u'plugged', u'plugs', u'plus', u'pocket', u'pockets', u'point', u'poor', u'poorly', u'port', u'portable', u'portraits', u'possesed', u'possibility', u'posted', u'potentially', u'power', u'practical', u'practically', u'practice', u'preferably', u'premium', u'prettier', u'pretty', u'prevents', u'previous', u'price', u'priced', u'pricing', u'prime', u'print', u'probably', u'problem', u'problems', u'procedure', u'procedures', u'produce', u'product', u'products', u'program', u'promised', u'prompt', u'promptly', u'properly', u'pros', u'protected', u'protection', u'protective', u'protector', u'protects', u'provide', u'provided', u'provides', u'ps3', u'psyched', u'puff', u'pull', u'purcashed', u'purchase', u'purchased', u'purchases', u'purchasing', u'purpose', u'push', u'pushed', u'quality', u'quick', u'quickly', u'quiet', u'quit', u'quite', u'qwerty', u'r450', u'randomly', u'range', u'rare', u'rate', u'rated', u'rating', u'razor', u'razr', u'reach', u'reaching', u'read', u'reading', u'ready', u'real', u'realize', u'really', u'reason', u'reasonable', u'reasonably', u'reboots', u'reccomendation', u'reccommend', u'receipt', u'receive', u'received', u'receiving', u'recently', u'reception', u'recessed', u'recharge', u'recieve', u'recognition', u'recognizes', u'recommend', u'recommended', u'red', u'refund', u'refurb', u'refuse', u'refused', u'regarding', u'regret', u'regretted', u'relative', u'relatively', u'reliability', u'remorse', u'removing', u'renders', u'reoccure', u'replace', u'replaced', u'replacement', u'replacementr', u'requirements', u'research', u'resistant', u'resolution', u'respect', u'rest', u'restart', u'restocking', u'restored', u'rests', u'results', u'return', u'returned', u'returning', u'reverse', u'reversible', u'review', u'reviews', u'ride', u'right', u'riingtones', u'ring', u'ringer', u'ringing', u'ringtones', u'rip', u'ripped', u'risk', u'roam', u'rocketed', u'rocks', u'roles', u'room', u'rotating', u'row', u'rubber', u'run', u'runs', u's11', u's710a', u'saggy', u'said', u'samsung', u'sanyo', u'satisfied', u'satisifed', u'save', u'saved', u'say', u'saying', u'says', u'scary', u'sch', u'scratch', u'scratched', u'screen', u'screens', u'seamlessly', u'searched', u'seat', u'seconds', u'secure', u'securely', u'securly', u'seeen', u'seen', u'self', u'seller', u'send', u'sending', u'sensitive', u'sensor', u'sent', u'seperated', u'series', u'seriously', u'service', u'set', u'setting', u'setup', u'severe', u'sex', u'shape', u'share', u'sharp', u'shield', u'shifting', u'shine', u'shiny', u'shipment', u'shipped', u'shipping', u'shooters', u'short', u'shots', u'shouldn', u'shouldve', u'shouting', u'shows', u'sides', u'sight', u'signal', u'signals', u'significantly', u'signs', u'sim', u'simple', u'simpler', u'simply', u'sins', u'sister', u'sitting', u'situations', u'size', u'sizes', u'sketchy', u'skip', u'skype', u'sleek', u'slid', u'slide', u'slider', u'sliding', u'slim', u'slipping', u'slow', u'slowly', u'small', u'smallest', u'smartphone', u'smell', u'smoke', u'smoking', u'smoother', u'smoothly', u'smudged', u'snap', u'snug', u'soft', u'software', u'sold', u'solid', u'somewhat', u'songs', u'sony', u'sound', u'sounded', u'sounds', u'speaker', u'spring', u'sprint', u'star', u'stars', u'started', u'starts', u'stated', u'static', u'station', u'stay', u'storage', u'store', u'strong', u'stuck', u'study', u'stuff', u'stupid', u'sturdiness', u'sturdy', u'styles', u'styling', u'stylish', u'submerged', u'sucked', u'sucks', u'sudden', u'suddenly', u'sunglasses', u'super', u'superb', u'superfast', u'supertooth', u'support', u'supposedly', u'suprised', u'sure', u'switch', u'takes', u'talk', u'tech', u'technology', u'tell', u'terrible', u'thank', u'thanks', u'thats', u'theory', u'thereplacement', u'thing', u'things', u'think', u'thorn', u'thought', u'threw', u'thumbs', u'tick', u'ticking', u'tied', u'tight', u'time', u'timeframe', u'timely', u'times', u'tinny', u'tiny', u'tips', u'tmobile', u'toactivate', u'toast', u'today', u'toilet', u'told', u'tone', u'tones', u'took', u'tool', u'tools', u'tooth', u'total', u'totally', u'touch', u'touches', u'tracfone', u'tracfonewebsite', u'tracking', u'transceiver', u'transfer', u'transformed', u'transmission', u'transmit', u'transmitters', u'trash', u'travled', u'tremendous', u'treo', u'tricky', u'tried', u'tries', u'trouble', u'truly', u'trunk', u'trust', u'try', u'trying', u'tungsten', u'turn', u'turned', u'turns', u'tv', u'type', u'ugly', u'unacceptable', u'unacceptible', u'unbearable', u'uncomfortable', u'understand', u'understanding', u'unfortunately', u'unhappy', u'unintelligible', u'unit', u'units', u'unknown', u'unless', u'unlike', u'unreliable', u'unsatisfactory', u'unusable', u'upbeat', u'update', u'upgrade', u'upload', u'upstairs', u'usable', u'usage', u'usb', u'use', u'used', u'useful', u'usefulness', u'useless', u'user', u'using', u'usually', u'utter', u'utterly', u'v1', u'v265', u'v325i', u'v3c', u'v3i', u'value', u've', u'vehicle', u'verizon', u'video', u'videos', u'virgin', u'visor', u'voice', u'voltage', u'volume', u'vx', u'vx9900', u'w810i', u'waaay', u'waiting', u'wake', u'walked', u'walkman', u'wall', u'wallet', u'want', u'wanted', u'warning', u'warranty', u'wasn', u'waste', u'wasted', u'wasting', u'waterproof', u'way', u'weak', u'wear', u'wearing', u'web', u'website', u'websites', u'week', u'weeks', u'weight', u'weird', u'went', u'whatsoever', u'whine', u'whistles', u'white', u'whoa', u'wi', u'wife', u'wild', u'wind', u'window', u'windows', u'winner', u'wiping', u'wire', u'wired', u'wirefly', u'wireless', u'wise', u'wish', u'wit', u'wobbly', u'won', u'wonder', u'wonderfully', u'wont', u'wood', u'wooden', u'word', u'work', u'worked', u'working', u'works', u'world', u'worn', u'worst', u'worth', u'worthless', u'worthwhile', u'wouldn', u'wow', u'wrong', u'wrongly', u'year', u'years', u'yell', u'yes']\n",
      "Feature counts: \n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# TODO: Instantiate a new CountVectorizer Using english stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "count_vectorized_review = count_vectorizer.fit_transform(X)\n",
    "\n",
    "print \"Feature names: \\n\", count_vectorizer.get_feature_names()\n",
    "print \"Feature counts: \\n\", count_vectorized_review.todense()\n",
    "print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Represent Count Vectorized results as a dataframe so we can preview it more easily.\n",
    "df2 = pd.DataFrame(\n",
    "    columns=count_vectorizer.get_feature_names(),\n",
    "    index=df.index,\n",
    "    data=count_vectorized_review.todense()\n",
    ")\n",
    "\n",
    "# Reminder: \"Stop words\" are non-content words.  (e.g. 'to', 'the', and 'it')\n",
    "# They aren’t helpful for prediction, so we remove them.\n",
    "# We'll almost always want to specify `stop_words = 'english'` to exclude stop words\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizers are like other models in `sklearn`:\n",
    "- We create a vectorizer object with the parameters of our feature space\n",
    "- We fit a vectorizer to learn the vocabulary\n",
    "- We transform a set of text into that feature space\n",
    "\n",
    "Note: there is a distinction between fit and transform:\n",
    "- We fit from our training set.  This is part of the model building process, so we don't look at our test set\n",
    "- We transform our test set using our model fit on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "While dense matrices store every entry in the matrix, sparse matrices only store the nonzero entries.  Sparse matrices don't have a lot of extra features, and some algorithms may not work for them so you use them when you need to work with matrices that would be too big for the computer to handle them, but they are mostly zero, so they compress easily.  You can convert from sparse matrices to dense matrices with `.todense()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "phone      168\n",
       "great       99\n",
       "good        77\n",
       "product     55\n",
       "quality     49\n",
       "headset     48\n",
       "works       47\n",
       "battery     46\n",
       "sound       43\n",
       "use         41\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q: What are the 10 most commonly used words in our training set?\n",
    "\n",
    "df2.sum(axis=0).sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split your data via train/test split using 30% of the dataset for training. Use `random_state=1` \n",
    "# so we can compare our results as a class\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([   1,    2,    5,   10,   11,   13,   15,   17,   18,   21,\n",
      "            ...\n",
      "            2879, 2883, 2885, 2888, 2894, 2925, 2930, 2934, 2935, 2937],\n",
      "           dtype='int64', length=1000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>wrongly</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yell</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  100  11  12  13 ...   wrongly  year  years  yell  yes\n",
       "2    0    0   0   0   0 ...         0     0      0     0    0\n",
       "5    0    0   0   0   0 ...         0     0      0     0    0\n",
       "10   0    0   0   0   0 ...         0     0      0     0    0\n",
       "11   0    0   0   0   0 ...         0     0      0     0    0\n",
       "\n",
       "[4 rows x 1000 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print X.index\n",
    "\n",
    "X[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   1,    2,    5,   10,   11,   13,   15,   17,   18,   21,\n",
       "            ...\n",
       "            2879, 2883, 2885, 2888, 2894, 2925, 2930, 2934, 2935, 2937],\n",
       "           dtype='int64', length=1000)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorized_review.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got estimator       10  100  11  12  13 ...   wrongly  year  years  yell  yes\n1      0    0   0   0   0 ...         0     0      0     0    0\n2      0    0   0   0   0 ...         0     0      0     0    0\n5      0    0   0   0   0 ...         0     0      0     0    0\n10     0    0   0   0   0 ...         0     0      0     0    0\n11     0    0   0   0   0 ...         0     0      0     0    0\n...   ..  ...  ..  ..  .. ...       ...   ...    ...   ...  ...\n2925   0    0   0   0   0 ...         0     0      0     0    0\n2930   0    0   0   0   0 ...         0     0      0     0    0\n2934   0    0   0   0   0 ...         0     0      0     0    0\n2935   0    0   0   0   0 ...         0     0      0     0    0\n2937   0    0   0   0   0 ...         0     0      0     0    0\n\n[1000 rows x 1000 columns]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-fce6ad0a3650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# df.sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Aerlinger/.pyenv/versions/2.7.11/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         cv = StratifiedShuffleSplit(stratify, test_size=test_size,\n",
      "\u001b[0;32m/Users/Aerlinger/.pyenv/versions/2.7.11/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Aerlinger/.pyenv/versions/2.7.11/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \"\"\"\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n",
      "\u001b[0;32m/Users/Aerlinger/.pyenv/versions/2.7.11/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Don't get num_samples from an ensembles length!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise TypeError('Expected sequence or array-like, got '\n\u001b[0;32m--> 112\u001b[0;31m                         'estimator %s' % x)\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got estimator       10  100  11  12  13 ...   wrongly  year  years  yell  yes\n1      0    0   0   0   0 ...         0     0      0     0    0\n2      0    0   0   0   0 ...         0     0      0     0    0\n5      0    0   0   0   0 ...         0     0      0     0    0\n10     0    0   0   0   0 ...         0     0      0     0    0\n11     0    0   0   0   0 ...         0     0      0     0    0\n...   ..  ...  ..  ..  .. ...       ...   ...    ...   ...  ...\n2925   0    0   0   0   0 ...         0     0      0     0    0\n2930   0    0   0   0   0 ...         0     0      0     0    0\n2934   0    0   0   0   0 ...         0     0      0     0    0\n2935   0    0   0   0   0 ...         0     0      0     0    0\n2937   0    0   0   0   0 ...         0     0      0     0    0\n\n[1000 rows x 1000 columns]"
     ]
    }
   ],
   "source": [
    "# df.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training a Classifier with Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a random forest model to predict \"sentiment\".\n",
    "\n",
    "*Use the Sklearn documentation as necessary for these steps!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Review Q: Why might we use a Random Forest model here instead of a decision tree?\n",
    "\n",
    "# YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Define a RandomForestClassifier that will train 20 decision trees.\n",
    "\n",
    "# YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Fit your RandomForestClassifier to your training data\n",
    "\n",
    "# YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 4: Evaluating your model using AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: The following code generate an ROC curve and calculates the corresponding AUC score.\n",
    "# You'll need to replace the appropriate variables to get this to run\n",
    "\n",
    "test_y_hat = rf_model.predict_proba(test_X_transformed)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, test_y_hat[:, 1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label = 'ROC curve (area = %0.4f)' % metrics.auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([.0, 1.])\n",
    "plt.ylim([.0, 1.1])\n",
    "plt.xlabel('FPR/Fall-out')\n",
    "plt.ylabel('TPR/Sensitivity')\n",
    "plt.title('Training Sentiment ROC')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure()\n",
    "# sns.distplot(fpr, color=\"red\", bins=10)\n",
    "# sns.distplot(tpr, color=\"green\", bins=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Now use the same process to generate an ROC score on the training dataset. Are you overfitting? \n",
    "# Why/why not? And if so, what can you do about it?\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Review Q: What is AUC score? Why are we using it?\n",
    "\n",
    "# YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Directions: Redo the analysis above with `TfidfVectorizer` instead of `CountVectorizer`. Use 10 estimators for your Random Forest Classifier.  What results do you get?\n",
    "\n",
    "(Check http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: YOUR CODE AND RESULTS HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q: What words have the highest Tf-Idf? What does this indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Questions/Exercises:\n",
    "\n",
    "- Which features are most important? (hint: Read the docs: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- Use `cross_val_score` instead of a simple train/test split. How much does your model improve?\n",
    "- In your own words*, describe what `cross_val_score` is doing\n",
    "- Try including larger n_grams (e.g. 2 or 3) in your analysis. Does this improve your results?\n",
    "- What other strategies can you use to improve your results?\n",
    "- Why might using KNN be a bad idea on this dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
